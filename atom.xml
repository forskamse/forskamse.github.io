<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Forskamse&#39;s Blog</title>
  
  
  <link href="https://forskamse.github.io/atom.xml" rel="self"/>
  
  <link href="https://forskamse.github.io/"/>
  <updated>2021-06-24T14:13:47.200Z</updated>
  <id>https://forskamse.github.io/</id>
  
  <author>
    <name>Forskamse</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>卷积神经网络详解(一)——基础知识</title>
    <link href="https://forskamse.github.io/2019/01/22/2019-01-22-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3(%E4%B8%80)%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <id>https://forskamse.github.io/2019/01/22/2019-01-22-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3(%E4%B8%80)%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</id>
    <published>2019-01-22T06:00:00.000Z</published>
    <updated>2021-06-24T14:13:47.200Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-卷积神经网络的组成"><a class="markdownIt-Anchor" href="#1-卷积神经网络的组成"></a> 1. 卷积神经网络的组成</h2><p>1981年诺贝尔医学奖得主，神经生物学家David Hubel 和Torsten Wiesel对人脑视觉系统的研究表明：人脑视觉系统首先通过眼睛来成像，图像通过瞳孔、晶状体最终在视网膜上成像。视网膜上布满了大量的光感受细胞，可以把光刺激转换为神经冲动，神经冲动通过视觉通路传递到大脑的初级视觉皮层（Primary Visual Cortex，V1），V1初步处理得到边缘、方向等特征信息，而后经由V2的进一步抽象得到轮廓、形状等特征信息，如此迭代地经由多层（V1层至V5层）的抽象后得到高层特征。高层特征是低层特征的组合<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>，从低层特征到高层特征的抽象过程中，语义的表现越来越清晰，存在的歧义越来越少，对目标的识别也就越来越精确。这就是人脑视觉系统的分层处理机制。</p><span id="more"></span><p>视觉皮层上的细胞有简单细胞（Simple Cell）与复杂细胞（Complex Cell）之分，这两种细胞的共同点是他们都只对特定方向的条形图样刺激有反应，而他们的主要区别是简单细胞对应的视网膜上的光感受细胞所在的区域比复杂细胞所对应的区域来得小，这个区域被称为感受野（Receptive Field）。这就是人脑视觉系统的感受野机制。</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190121231252850_1844689380.png"></div><p>1980年，日本学者Kunihiko Fukushima提出感知机模型（Neocognitron），提出使用卷积层来模拟视觉细胞对特定图案的反应、使用池化层模拟感受野的方法。卷积神经网络的设计深受这个方法的影响，其基本结构为：</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190107202305429_94794149.png"></div><p>具体说来，卷积层用于提取不同的图像特征，有减少参数数量、保留空间信息的作用；池化层用于模拟感受野，有选取特征、减少参数数量的作用，同时引入微小平移不变性<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>；而激活层的设置则是为了引入非线性因子，提升模型的表达能力，这是神经网络中普遍采用的。</p><h2 id="2-卷积层"><a class="markdownIt-Anchor" href="#2-卷积层"></a> 2. 卷积层</h2><h3 id="21-图像的局部相关性"><a class="markdownIt-Anchor" href="#21-图像的局部相关性"></a> 2.1 图像的局部相关性</h3><p>图像是具有局部相关性的一类数据<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>，其局部相关性是指组成图像的每个像素点与其周围的像素点是有关联的，而图像上距离较远的像素相关性较弱，因此处理图像时实际上没必要每个神经元都对全局图像进行感知。</p><h3 id="22-全连接网络用于图像处理"><a class="markdownIt-Anchor" href="#22-全连接网络用于图像处理"></a> 2.2 全连接网络用于图像处理</h3><p>以MNIST手写数字识别为例，该数据集中的图像为（28，28，1）的灰度图像，这个图像由28 * 28个像素点（Pixel）构成，每个像素点有一个通道（Channel）。如果使用全连接网络（即网络中的神经元与相邻层上的每个神经元均连接），那么输入层有28 * 28 =784个神经元，假设hidden层采用了15个神经元<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>，输出层是10个神经元，那么参数个数(w和b)就有：784 * 15 * 10+15+10=117625个。即使在这种情况下，参数量都十分庞大了，如果输入图像的像素点更多、全连接网络的隐藏层层数更多、隐藏层神经元数量更多，参数量就会更加庞大。<br />大量的参数很容易导致网络过拟合，而且每进行一次反向传播计算量都是巨大的，无论从调参还是计算资源的角度都不建议用全连接网络做图像处理。此外，全连接网络认为“每个输入值都是平等的”，它将输入视为一维向量，并不关心这个像素是第几行、第几列的像素，忽视了空间信息，用于图像这种具有空间局部相关性的数据也是不合适的。</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/1546681571_28983.png"></div><h3 id="23-减少参数数量"><a class="markdownIt-Anchor" href="#23-减少参数数量"></a> 2.3 减少参数数量</h3><p>在图像局部相关性的支撑下，卷积连接应用而生。<br />为简化说明，来看一个简单的例子：<br />在下面的这张图中，输入为3 * 3 = 9个像素，如果将其与16个隐藏层神经元全连接，就会有9 * 16 = 144个连接，也就有144个权值。</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190112180923731_700883300.png"></div><p>为减少连接数，并且基于图像局部相关性的假设，可以仅取四个位置相近（注意图像的Width和Height两个维度）的像素作为输入，四个像素与同一个神经元进行连接，连接的权值记为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">w_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">w_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>， 如下图所示。</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190112181611409_233506424.png"></div><p>这种连接方式又可看作是数学上的卷积操作<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>，其中这一组权值就被称为卷积核， 如下图右图所示，因此得名卷积连接。</p><p>我们将这个卷积操作在输入图像上滑动起来，自然地，连接的神经元也向下滑动，连接的权值仍记为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">w_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">w_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，如下图所示。此时注意，这个卷积操作要覆盖图像上的所有9个像素，需要滑动四次，因此对应着四个神经元。图像的四部分局部像素与这四个神经元连接时共享同一套权值（简洁地说，这四个神经元共享一套权值），这就是所谓的“权值共享”的概念。这组权值又叫做卷积核。</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190112183300720_471577742.png"></div><p>卷积连接方式使得每个神经元所感知的图像的范围由整张图缩减到了4个像素点，从而减少了权值的数量，又采取了“权值共享”的方法，进一步减少了参数的数量。</p><p>允许我们对图像进行卷积操作的理论依据就是图像的局部相关性：卷积神经网络的设计认为每个神经元没必要对全局图像进行感知，只需对局部像素按空间位置进行局部连接即可。</p><h3 id="24-提取图像特征"><a class="markdownIt-Anchor" href="#24-提取图像特征"></a> 2.4 提取图像特征</h3><p>卷积层每次用一个卷积核在图像上滑动，来提取图像的某一显著特征。<br />卷积核可以找到图中和卷积核自身最相似的部分，而且相似度越高，得到的响应值越大。</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190122174325180_288459116.png"></div><p>图7中上面一排的照片是5架战斗机，把其中一架战斗机的图像截出来作为卷积核，与原图像进行卷积，得到结果如下排图像所示。可以看到，每架战斗机所在的位置都得到了一个极大的响应。<br />因此，通过设置合理的损失函数，在卷积神经网络中使用反向传播算法，最终可以学习到相应于目标结果的卷积核，在Inference的时候就可以提取出有效特征。</p><h3 id="25-保留空间信息"><a class="markdownIt-Anchor" href="#25-保留空间信息"></a> 2.5 保留空间信息</h3><p>与全连接网络相比，卷积网络没有将图像展开为一维向量，而是使用卷积核在原图像上滑动来提取特征，因此保留了原图像的局部空间信息。<br />图7中上面一排右边的照片是由左边照片对5架战斗机平移得到的。经过同样的卷积操作后，得到的特征响应图相当于左边的特征响应图做相应的平移。这是卷积神经网络的局部连接和权值共享带来的“同变性（Equivariance）”<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>，亦是卷积神经网络可以保留图像空间信息的体现。</p><h3 id="26-卷积操作的补充"><a class="markdownIt-Anchor" href="#26-卷积操作的补充"></a> 2.6 卷积操作的补充</h3><ol><li>卷积核（Filter）</li></ol><p>在数学定义上，矩阵的卷积（Convolution）操作为：首先将卷积核进行翻转，构成一个卷积核的镜像，然后使用该镜像和前面矩阵相应位置进行点乘。如下面所示：</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/1546851390_24790.png"></div><ol start="2"><li>步长（Step）</li></ol><p>卷积操作每次移动的单位数称为步长。</p><ol start="3"><li>填充（Padding）<br />为了控制输出的尺寸，可以采用填充的方法。例如在步长为2的情况下，输出尺寸原为5 * 5，如果想使输出尺寸为3 * 3，  可以在输入外围添加一圈0，在这种情况下，输出的尺寸就是3 * 3。</li></ol><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190107180531456_2081156539.png"></div><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190112180202610_1822722602.png"></div><h2 id="3-激活层"><a class="markdownIt-Anchor" href="#3-激活层"></a> 3. 激活层</h2><p>激活层算不上卷积神经网络的特色，这里就不详细介绍了，简而言之，激活层的作用就是引入非线性因子，提升模型的表达能力。</p><h2 id="4-池化层"><a class="markdownIt-Anchor" href="#4-池化层"></a> 4. 池化层</h2><p>卷积神经网络在卷积层和激活层之后又增加了池化层，用来模拟感受野，以达到选取特征、减少参数数量的作用，同时引入微小平移不变性。</p><h3 id="41-特征选取"><a class="markdownIt-Anchor" href="#41-特征选取"></a> 4.1 特征选取</h3><p>池化的一个功能是对特征的选取，卷积神经网络中常用的有Average Pooling和Maximum Pooling。Average Pooling，即对池化区域内特征点求平均，Maximum Pooling则对池化区域内特征点取最大。Average Pooling更能保留图片的背景信息，如果背景中也含有有效信息，Average Pooling就更合适；Maximum Pooling会忽略背景信息，在有噪声的情况下则更有效。<br />通过池化，CNN进一步减少了参数数量（降维）。</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/1540565865_21442.png"></div><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/1540565888_8954.png"></div><h3 id="42-微小平移不变性"><a class="markdownIt-Anchor" href="#42-微小平移不变性"></a> 4.2 微小平移不变性</h3><p>在局部连接和权值共享的作用下，平移后图像的特征映射图与特征映射图直接做对应的平移得到的结果差别不大，即前面所述的同变性。</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190107204820550_1059782468.png"></div>此时再进行池化，以Maximum Pooling为例，如下图所示，在左图中得到的池化结果是11，在右图中得到的池化结果也是11，体现了平移不变性。  <div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190108100749906_1800606002.png"></div>需要指出的是，池化的平移不变性是有限的，即所说的微小平移不变性。如果平移超出了感受野的位置，平移不变性就难以体现。  <h2 id="5-分层表达"><a class="markdownIt-Anchor" href="#5-分层表达"></a> 5. 分层表达</h2><p>前已述及，人脑视觉系统存在分层处理的机制，卷积神经网络用多层的网络来模拟人脑视觉系统的分层处理。<br />通过多层的卷积神经网络，计算机逐步“理解”一幅图像大致遵循这样的过程：像素–&gt;边缘–&gt;基本形状–&gt;复杂图案–&gt;更复杂图案。<br />例如，在学习一张车的图片时，浅层的卷积层能从最基本的像素中学习到边缘特征，较深层点的可以学习到圆形等基本形状，再经过几层可以学习到轮胎、车身等图案特征，最后可以学习到车的整体特征。</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190108113541696_1346990095.png"></div><h2 id="6-卷积的可视化与解释性"><a class="markdownIt-Anchor" href="#6-卷积的可视化与解释性"></a> 6. 卷积的可视化与解释性</h2><p>深度学习的解释性学界仍在研究当中。目前对于卷积神经网络，仅能通过可视化提供简单的解释。</p><h3 id="61-边缘检测"><a class="markdownIt-Anchor" href="#61-边缘检测"></a> 6.1 边缘检测</h3><p>先解释卷积层如何做边缘检测。<br />图片最常做的边缘检测有两类：垂直边缘（Vertical Edges）检测和水平边缘（Horizontal Edges）检测。</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190108120127391_1249309616.png"></div> <p>以垂直边缘检测为例，原始灰度图像尺寸为 6x6，卷积核尺寸为 3x3，不做Padding，Stride = 1，卷积后得到的特征映射图尺寸为 4x4，得到结果如下：</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190108142020333_337721441.png"></div> 在灰度图像中，０代表灰，正值表示白，值越大越白，负值代表黑，值越小越黑。本例的原始图像和卷积后的特征映射图，有图例所示的黑白灰分布，提取出了垂直边缘。  <h3 id="62-响应相似图形"><a class="markdownIt-Anchor" href="#62-响应相似图形"></a> 6.2 响应相似图形</h3><p>这部分可以参考2.4节。</p><h3 id="63-特征响应图可视化"><a class="markdownIt-Anchor" href="#63-特征响应图可视化"></a> 6.3 特征响应图可视化</h3><p>相关研究<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>中，曾实现了对特征映射图的可视化。直接看一下结果：</p><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190108144217752_522028051.png"></div><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190108144230894_1788507434.png"></div><div align="center"><img  src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jan_2019/CNN_Basics_P1/20190108144335133_1669012016.png"></div>目前的研究虽然还是不能完全解释CNN，但是通过可视化，我们发现CNN学习到的特征确实如我们所预期的呈现分层特性，底层是一些边缘角点以及颜色的抽象特征，越到高层则越呈现出具体的特征，与人类视觉系统类似。  <h2 id="7-卷积神经网络为什么有效"><a class="markdownIt-Anchor" href="#7-卷积神经网络为什么有效"></a> 7. 卷积神经网络为什么有效</h2><ol><li>从神经科学角度：卷积神经网络模仿了人脑视觉系统的分层处理机制以及感受野机制；</li><li>从统计角度：卷积神经网络抓住了图像的局部相关性（Spatially-local Correlation）；</li><li>从正则化的角度：由于局部连接、权值共享和池化，降低了模型参数数量，控制了模型复杂度，可有效避免模型过拟合。</li></ol><p><strong>关键词 ： 局部感受野、权值共享、时间/空间亚采样</strong></p><h2 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h2><p><a href="https://book.douban.com/subject/27125397/">深度学习与计算机视觉——算法原理、框架应用与代码实现</a><br /><a href="https://www.linkedin.com/pulse/derivation-convolutional-neural-network-from-fully-connected-gad">Derivation of Convolutional Neural Network from Fully Connected Network Step-By-Step</a><br /><a href="https://www.cnblogs.com/charlotte77/p/7759802.html">卷积神经网络CNN原理详解(一)——基本原理</a><br /><a href="https://lguduy.github.io//2017/07/02/CNN%E4%B8%BA%E4%BB%80%E4%B9%88work/#annotations:EasaThA1EemS5wtu41br7w">CNN为什么有效</a><br /><a href="https://zhangting2020.github.io/2018/05/30/Transform-Invariance/">卷积神经网络为什么具有平移不变性？</a><br /><a href="https://zhuanlan.zhihu.com/p/30800318">吴恩达 DeepLearning.ai 课程提炼笔记（4-1）卷积神经网络 — 卷积神经网络基础</a><br /><a href="https://blog.csdn.net/zhang2012liang/article/details/52687498">CNN十大问</a><br /><a href="https://www.cnblogs.com/hejunlin1992/p/7583444.html">CNN中减少网络的参数的三个思想</a></p><hr class="footnotes-sep" /><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>需要注意的是这些层之间的连接并不一定是简单的逐层连接，例如V2层就与其它层均有连接，对这些层之间连接的进一步研究有可能进一步提升卷积神经网络的效果。 <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>亦有人称池化同时引入了尺度变换和旋转的不变性，由于其可解释性与效果都不佳，因此认可度不高，在此省略不讲。 <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>语音和自然语言也是具有局部特征性的数据，CNN也可以用于语音处理与自然语言处理。 <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>可看作选择15个特征。 <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>实际上应该是数学上的互相关（Cross-correlation）。在深度学习中，我们使用的卷积运算实则没有卷积核翻转为镜像的这一步操作，因为在权重学习的角度，翻转是没有必要的，互相关与卷积相差的就是核没有翻转，所以深度学习中的卷积操作在数学上准确度来说称为互相关。 <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p><a href="https://blog.csdn.net/voxel_grid/article/details/79275637#annotations:l5eZ9BJ8EemTjb9M9uq6Mw">关于 CNN对图像特征的 位移、尺度、形变不变性的理解</a> <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p><a href="https://zhuanlan.zhihu.com/p/24833574">Deep Visualization:可视化并理解CNN</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-卷积神经网络的组成&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-卷积神经网络的组成&quot;&gt;&lt;/a&gt; 1. 卷积神经网络的组成&lt;/h2&gt;
&lt;p&gt;1981年诺贝尔医学奖得主，神经生物学家David Hubel 和Torsten Wiesel对人脑视觉系统的研究表明：人脑视觉系统首先通过眼睛来成像，图像通过瞳孔、晶状体最终在视网膜上成像。视网膜上布满了大量的光感受细胞，可以把光刺激转换为神经冲动，神经冲动通过视觉通路传递到大脑的初级视觉皮层（Primary Visual Cortex，V1），V1初步处理得到边缘、方向等特征信息，而后经由V2的进一步抽象得到轮廓、形状等特征信息，如此迭代地经由多层（V1层至V5层）的抽象后得到高层特征。高层特征是低层特征的组合&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn1&quot; id=&quot;fnref1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;，从低层特征到高层特征的抽象过程中，语义的表现越来越清晰，存在的歧义越来越少，对目标的识别也就越来越精确。这就是人脑视觉系统的分层处理机制。&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://forskamse.github.io/categories/Deep-Learning/"/>
    
    <category term="Neural Networks" scheme="https://forskamse.github.io/categories/Deep-Learning/Neural-Networks/"/>
    
    <category term="CNN Special Column" scheme="https://forskamse.github.io/categories/Deep-Learning/Neural-Networks/CNN-Special-Column/"/>
    
    
    <category term="CNN" scheme="https://forskamse.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>打包TensorFlow Object Detection API</title>
    <link href="https://forskamse.github.io/2018/07/24/2018-07-24-%E6%89%93%E5%8C%85TensorFlow%20Object%20Detection%20API/"/>
    <id>https://forskamse.github.io/2018/07/24/2018-07-24-%E6%89%93%E5%8C%85TensorFlow%20Object%20Detection%20API/</id>
    <published>2018-07-24T08:27:57.000Z</published>
    <updated>2021-06-24T14:14:08.334Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题来源"><a class="markdownIt-Anchor" href="#问题来源"></a> 问题来源</h2><p>对于TensorFlow Object Detection API我个人有两点不满：</p><ol><li>安装： <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md">官方安装指导</a>要求使用者先克隆下整个models仓库，然后安装Protobuf，编译解压出object_detection模块。最后还要将object_detection模块与slim模块的目录export到PYTHONPATH中，这一操作在新打开Shell后都需要重新进行（当然，添加到环境变量配置文件中可以免除此麻烦）。整个过程是十分繁琐的。特别的，在Windows系统上，安装配置protobuf相对麻烦（安装指导中就没有给出Windows平台的情况），同时还存在bug，识别一些模型时会出问题【参见 <a href="https://github.com/tensorflow/tensorflow/issues/3002">Issue3002</a>】。</li></ol><span id="more"></span><ol start="2"><li>使用：在object_detection的ipython tutorial中，使用了如下语句：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from utils import label_map_util, visualization_utils</span><br></pre></td></tr></table></figure><p>这里的utils是object_detection下的子模块。如果按照这个写法，运行脚本不在object_detection目录下就无法找到utils。</p><h2 id="解决方法"><a class="markdownIt-Anchor" href="#解决方法"></a> 解决方法</h2><p>鉴于以上问题，我将object detection api打包成whl包和egg包，直接使用pip或者easy_install安装即可。</p><p>我将打包好的whl包与egg包分享出来，打包环境为Raspbian 9 Stretch（Python环境是3.5.3和Python2.7.13）、Win10 64位（Python环境是3.6.6和Python2.7.15）、Manjaro Linux 64位（Python环境是3.6.6和Python2.7.15）。下载地址：<a href="https://github.com/forskamse/Pyhton_Packages_of_TensorFlow_Object_Detection_API">Pyhton_Packages_of_TensorFlow_Object_Detection_API</a></p><h4 id="1-安装依赖"><a class="markdownIt-Anchor" href="#1-安装依赖"></a> 1. 安装依赖</h4><p>Tensorflow Object Detection API 需要如下依赖：</p><p>[使用包管理工具安装（Linux）/系统级安装（Windows）]</p><ul><li>protobuf</li><li>python-tk (tk in Manjaro)<br />[使用pip安装]</li><li>tensorflow</li><li>pillow</li><li>lxml</li><li>jupyter</li><li>matplotlib</li><li>cython</li><li>contextlib2</li></ul><h4 id="2-安装下载的whl包或者egg包"><a class="markdownIt-Anchor" href="#2-安装下载的whl包或者egg包"></a> 2. 安装下载的whl包或者egg包</h4><p>安装whl只需执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install xxx.whl</span><br></pre></td></tr></table></figure><p>安装egg只需执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">easy_install xxx.egg</span><br></pre></td></tr></table></figure><h4 id="3-使用"><a class="markdownIt-Anchor" href="#3-使用"></a> 3. 使用</h4><p>使用时，请使用标准的引用Python模块的子模块的用法，对于object_detection的utils或是其他子模块，请使用：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from object_detection.utils import label_map_util, visualization_utils</span><br></pre></td></tr></table></figure><p>这样运行脚本不管放在哪里，都是可以搜索到utils并执行的。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;问题来源&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#问题来源&quot;&gt;&lt;/a&gt; 问题来源&lt;/h2&gt;
&lt;p&gt;对于TensorFlow Object Detection API我个人有两点不满：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;安装： &lt;a href=&quot;https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md&quot;&gt;官方安装指导&lt;/a&gt;要求使用者先克隆下整个models仓库，然后安装Protobuf，编译解压出object_detection模块。最后还要将object_detection模块与slim模块的目录export到PYTHONPATH中，这一操作在新打开Shell后都需要重新进行（当然，添加到环境变量配置文件中可以免除此麻烦）。整个过程是十分繁琐的。特别的，在Windows系统上，安装配置protobuf相对麻烦（安装指导中就没有给出Windows平台的情况），同时还存在bug，识别一些模型时会出问题【参见 &lt;a href=&quot;https://github.com/tensorflow/tensorflow/issues/3002&quot;&gt;Issue3002&lt;/a&gt;】。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://forskamse.github.io/categories/Deep-Learning/"/>
    
    <category term="Deployment" scheme="https://forskamse.github.io/categories/Deep-Learning/Deployment/"/>
    
    
    <category term="Deep Learning" scheme="https://forskamse.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux远程桌面服务VNC/XRDP/Xdmcp/SSH+X11转发及其在树莓派上的使用</title>
    <link href="https://forskamse.github.io/2018/07/21/2018-07-12-Linux%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%85%B6%E5%9C%A8%E6%A0%91%E8%8E%93%E6%B4%BE%E4%B8%8A%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>https://forskamse.github.io/2018/07/21/2018-07-12-Linux%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%85%B6%E5%9C%A8%E6%A0%91%E8%8E%93%E6%B4%BE%E4%B8%8A%E7%9A%84%E4%BD%BF%E7%94%A8/</id>
    <published>2018-07-21T08:37:57.000Z</published>
    <updated>2021-06-24T14:23:09.658Z</updated>
    
    <content type="html"><![CDATA[<p>Linux下有三大知名的远程桌面服务，即VNC/XRDP/Xdmcp，此外还有一个认知度不那么高的SSH+X11转发服务也是很好用的。下面的介绍中我引入在树莓派上的应用（使用Raspbian Stretch），给大家一个直观的认识。</p><span id="more"></span><h2 id="vnc"><a class="markdownIt-Anchor" href="#vnc"></a> VNC</h2><p>使用VNC服务时，先在树莓派上安装vncserver，然后在PC或其他设备上安装vncviewer。</p><p>树莓派上的vncserver有：realvnc、tightvnc、x11vnc等。其实知名的vnc服务提供方还有tigervnc和ultravnc等，只是没有推出arm或树莓派版本。</p><p>realvnc在此下载安装：<a href="https://www.realvnc.com/en/connect/download/vnc/raspberrypi/">https://www.realvnc.com/en/connect/download/vnc/raspberrypi/</a><br />tightvnc和x11vnc使用apt-get安装即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install tightvnc</span><br><span class="line">或</span><br><span class="line">sudo apt-get install x11vnc</span><br></pre></td></tr></table></figure><p>realvnc和tightvnc在安装完成后执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vncserver</span><br></pre></td></tr></table></figure><p>首次执行，会要求输入密码以及view-only模式密码，输入后生成一个桌面，提示如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">New desktop is raspberrypi:1 (192.168.253.6:1)</span><br></pre></td></tr></table></figure><p>x11vnc在安装完成后执行:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#设置密码</span><br><span class="line">x11vnc -storepasswd</span><br><span class="line">#启动服务</span><br><span class="line">x11vnc -auth guess -once -loop -noxdamage -repeat -rfbauth /home/pi/.vnc/passwd -rfbport 5900 -shared</span><br></pre></td></tr></table></figure><p>vncviewer可以使用realvnc、tightvnc、ultravnc、tigervnc等提供的vncviewer。</p><p>realvnc viewer：<a href="https://www.realvnc.com/en/connect/download/viewer/">https://www.realvnc.com/en/connect/download/viewer/</a></p><p>realvnc提供的vncviewer支持很多设备，同时复制粘贴等功能也相对完善，一般选用realvnc viewer就可以了。<br /><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/Remote_Desktop_Service/2018071200151253.jpeg" alt="" /></p><p>tightvnc viewer：<a href="https://www.tightvnc.com/download.php">https://www.tightvnc.com/download.php</a></p><p>ultravnc viewer：<a href="http://www.uvnc.com/downloads.html">http://www.uvnc.com/downloads.html</a></p><p>tigervnc viewer：<a href="https://bintray.com/tigervnc/beta/tigervnc">https://bintray.com/tigervnc/beta/tigervnc</a></p><h2 id="xrdp"><a class="markdownIt-Anchor" href="#xrdp"></a> XRDP</h2><p>Windows上有一个远程桌面服务（Remote Desktop Protocol，RDP），Linux上类似的RDP服务称为XRDP。</p><p>安装方法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install xrdp</span><br></pre></td></tr></table></figure><p>XRDP安装完是默认启动的，开机也会自动启动。</p><p>Windows上自带的远程桌面连接可以直接使用：</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/Remote_Desktop_Service/2018071200491873.png" alt="" /></p><p>不过复制粘贴这些功能做的不如vncviewer好。</p><h2 id="xdmcp"><a class="markdownIt-Anchor" href="#xdmcp"></a> Xdmcp</h2><p>Xdmcp（X Display Manager Control Protocol），即X显示管理器控制协议，由DP（Display Manager），即显示管理器。</p><p>树莓派上默认使用的是lightdm这个显示管理器，修改其配置以启用Xdmcp：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/lightdm/lightdm.conf</span><br><span class="line">#找到XDMCP Server configuration，修改启用项配置，其他端口等配置不必改。</span><br><span class="line">enabled=true</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/Remote_Desktop_Service/20180712011957109.jpeg" alt="" /></p><p>Xdmcp的客户端方面，我推荐xmanager，下载地址：<a href="http://www.xshellcn.com/xiazai.html">http://www.xshellcn.com/xiazai.html</a></p><p>建立会话：</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/Remote_Desktop_Service/20180712013335521.jpeg" alt="" /></p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/Remote_Desktop_Service/20180712013344447.jpeg" alt="" /></p><p>此外，也可以使用Mobaxterm：</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/Remote_Desktop_Service/20180712023253542.jpeg" alt="" /></p><h2 id="sshx11转发"><a class="markdownIt-Anchor" href="#sshx11转发"></a> SSH+X11转发</h2><p>这种远程桌面服务方式在服务端方面不需要进行更多的设置，只要SSH能正常访问即可。</p><p>客户端方面推荐使用Mobaxterm，只需要在对话中将Remote environment由Interactive shell修改为LXDE desktop，如下：</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/Remote_Desktop_Service/20180712020827542.jpeg" alt="" /></p><p>打开后的效果如下：</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/Remote_Desktop_Service/20180712020545402.jpeg" alt="" /></p><p>使用Putty也是可以的，在会话设置的Connection–SSH–X11下启用X11 forwarding：</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/Remote_Desktop_Service/20180712022851820.png" alt="" /></p><p>打开终端后，执行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">startlxde</span><br></pre></td></tr></table></figure><p>就可以打开桌面了。</p><p>说一句题外话，Mobaxterm是个十分全面的终端软件，无论是明码文字接口Telnet、Rsh，密码文字接口SSH，图形接口Xdmcp（XServer）、RDP（XRDP）、VNC，X11 Forwarding，文件传输FTP、SFTP，甚至是串口Serial都支持。本人强烈推荐。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Linux下有三大知名的远程桌面服务，即VNC/XRDP/Xdmcp，此外还有一个认知度不那么高的SSH+X11转发服务也是很好用的。下面的介绍中我引入在树莓派上的应用（使用Raspbian Stretch），给大家一个直观的认识。&lt;/p&gt;</summary>
    
    
    
    <category term="Computer" scheme="https://forskamse.github.io/categories/Computer/"/>
    
    <category term="Linux" scheme="https://forskamse.github.io/categories/Computer/Linux/"/>
    
    <category term="Raspberry Pi" scheme="https://forskamse.github.io/categories/Raspberry-Pi/"/>
    
    
    <category term="Computer" scheme="https://forskamse.github.io/tags/Computer/"/>
    
    <category term="Operation&amp;Maintenance" scheme="https://forskamse.github.io/tags/Operation-Maintenance/"/>
    
    <category term="Raspberry Pi" scheme="https://forskamse.github.io/tags/Raspberry-Pi/"/>
    
  </entry>
  
  <entry>
    <title>磁盘知识</title>
    <link href="https://forskamse.github.io/2018/07/21/2018-07-21-%E7%A3%81%E7%9B%98%E7%9F%A5%E8%AF%86/"/>
    <id>https://forskamse.github.io/2018/07/21/2018-07-21-%E7%A3%81%E7%9B%98%E7%9F%A5%E8%AF%86/</id>
    <published>2018-07-21T08:37:57.000Z</published>
    <updated>2021-06-24T14:22:42.088Z</updated>
    
    <content type="html"><![CDATA[<p>注：下文不特别区分“磁盘”、“硬盘”两个概念，认为“固态硬盘”也属于“磁盘”的一种。</p><h2 id="磁盘接口"><a class="markdownIt-Anchor" href="#磁盘接口"></a> 磁盘接口</h2><p>IDE（Integrated Drive Electronics），即电子集成驱动器，是曾经主流的磁盘接口（同时也作为光驱的接口）。IDE又称ATA（Advanced Technology Attachment），即“高级技术附件”，在SATA（Serial ATA）出现后改名为PATA（Parallel ATA）。</p><p>SCSI（Small Computer System Interface），即小型计算机系统接口，可以连接硬盘、软驱、光驱、打印机等设备。SCSI出现的原因主要是因为IDE接口的硬盘转速太慢，传输速率太低。从原理层面看，SCSI与IDE一样使用的是并行技术，因此在SAS（Serial Attached SCSI）出现后SCSI就通常被称为并行SCSI了。</p><p>此后又出现了SATA（Serial Advanced Technology Attachment，串行高级技术附件）、SAS（Serial Attached SCSI）即串行连接SCSI 等接口，使用了串行技术，提高了数据传输速率。</p><p>我们可以将，SCSI和SAS划归为另一个系列，SCSI和SAS价格较高，在各自的年代都是用在高级的服务器上的，私人电脑上较少使用。而将IDE和SATA划归为一个系列，它们用于一般电脑或PC上，SATA在当下非常普遍。</p><span id="more"></span><h2 id="命名"><a class="markdownIt-Anchor" href="#命名"></a> 命名</h2><p>在Linux上，IDE硬盘会被标识为hd，例如，在一个IDE接口上接着的两块硬盘会被分别标识为hda与hdb；出现SCSI硬盘后，Linux将其标识为sd，如sda、sdb。此后所有除IDE接口外的硬盘全部沿用SCSI硬盘的标识标准，即sd，没有再做改动。</p><h2 id="机械硬盘与固态硬盘"><a class="markdownIt-Anchor" href="#机械硬盘与固态硬盘"></a> 机械硬盘与固态硬盘</h2><h4 id="机械硬盘组成与容量"><a class="markdownIt-Anchor" href="#机械硬盘组成与容量"></a> 机械硬盘组成与容量</h4><p>机械硬盘由盘片、磁头及其它辅助机构组成。盘片（Disk）是硬盘中承载数据存储的介质，其上附着着磁粉，磁粉的S/N极将分别代表着二进制中的0和1，从而表示二进制数据。利用磁头（Head）的磁力控制指定的一些磁粉的方向，就存储了特定的信息。</p><p>下图是一个盘片的示意图：<br /><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/About_Disks/20180721165231268.gif" alt="" /></p><p>如上图所示，盘片被分为很多条磁道（Track），即表面上的一些同心圆，磁道是从盘片外圈往内圈编号0磁道，1磁道…。每一个磁道又按512个字节为单位划分为等分，叫做扇区（Sector）。</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/About_Disks/20180721165302175.gif" alt="" /></p><p>磁盘是由多个盘片叠加在一起，互相之间由垫圈隔开。盘片上下两面各有一个磁头，每张盘片同一位置的磁道，组成了柱面（Cylinder ）。显然，磁道数=柱面数。</p><p>知道CHS（Cylinders、Heads、Sectors）的数量后，就可以确定磁盘的容量：</p><p><em>磁盘容量 = 柱面数（磁道数）× 磁头数 × 扇区数 × 扇区大小（512 Bytes）</em></p><h4 id="固态硬盘"><a class="markdownIt-Anchor" href="#固态硬盘"></a> 固态硬盘</h4><p>固态硬盘没有采用磁性介质作为存储介质，而是使用半导体材料来存储数据，但“磁盘”这个词还是沿用在了固态硬盘上。</p><p>固态硬盘的内部构造十分简单，固态硬盘内主体其实就是一块PCB板，而这块PCB板上最基本的配件就是控制芯片，缓存芯片（部分低端硬盘无缓存芯片）和用于存储数据的闪存芯片。如下图所示：</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/About_Disks/20181030172815673.png" alt="在这里插入图片描述" /></p><p>固态硬盘也有扇区（Sector）的概念，与机械硬盘一致。</p><h2 id="分区表初始分区"><a class="markdownIt-Anchor" href="#分区表初始分区"></a> 分区表（初始分区）</h2><p>之所以介绍一下磁盘的基础知识，是为了让大家更好地理解这一部分对硬盘初始分区的介绍。一块全新的硬盘，必须进行初始分区。初始分区可以分为MBR分区和GPT分区两种形式，对应MBR分区表和GPT分区表。</p><h4 id="mbr分区形式"><a class="markdownIt-Anchor" href="#mbr分区形式"></a> MBR分区形式</h4><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/About_Disks/20180721165337842.jpeg" alt="" /></p><p>如上图所示，MBR分区形式下，硬盘的第一个扇区是主引导扇区，由三个部分组成：主引导记录（Master Boot Record，MBR）、硬盘分区表（Disk Partition Table，DPT）和硬盘有效标志。</p><p>MBR占446个字节，负责从活动分区（活动分区是即启动分区，安装有系统）中装载并运行系统引导程序。</p><p>DPT占64个字节，记录着硬盘中分区的数量以及每一分区的大小，每个分区的信息用16个字节表示，因此限制了分区的数量：不能超过4个主分区或者3主分区+1扩展分区，而扩展分区可以划分为任意数量的逻辑分区（扩展分区不可直接使用，需要转化为逻辑分区方可使用）【注1：逻辑分区数量受系统层级的限制，在Linux中，IDE硬盘最多有59个逻辑分区（5-63），SATA硬盘则有11个逻辑分区（5-15）】。 这16字节的具体内容是：启动标志、起止磁头号、起止扇区号、起止柱面号、隐含扇区数目(4个字节)、分区总扇区数目(4个字节)。这里又暴露了MBR分区的一个缺陷：用4个字节表示分区总扇区数， 最大能表示2的32次方的扇区个数，按每扇区512字节计算，每个分区最大不能超过2.2TB。</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/About_Disks/20180721165409994.jpeg" alt="" /></p><p>硬盘有效标志占2个字节，又被称为幻数（Magic Number），固定为55AA。如果该标志错误系统就不能启动。</p><h4 id="gpt分区形式"><a class="markdownIt-Anchor" href="#gpt分区形式"></a> GPT分区形式</h4><p>GPT是GUID（Globally Unique Identifier Partition Table） Partition Table的缩写，意即全局唯一标识符分区表。</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/About_Disks/20180721165427999.png" alt="" /></p><p>GPT分区的LBA0是保护性MBR【注2：LBA意为逻辑块地址（Logical Bolck Address），每个逻辑块的大小是512字节，这是与扇区不同的定位方式 】，为了实现有限的兼容性，GPT仍然为MBR保留了这一扇区，用来阻止基于MBR的磁盘工具识别错误，从而覆盖GPT磁盘。</p><p>LBA1是GPT头（Primary GPT Header），LBA-1是备份GPT头（Secondary GPT Header），这两部分内容是一样的。GPT头的具体内容在此不做详细说明了。</p><p>LBA2-LBA33是分区表项，LBA-33 - LBA-2是备份分区表项，两部分内容也是一样的。GPT就是为了避免MBR的两大缺点：在GPT中，分区表项的数量有MBR的4项增多到128项，因此允许划分128个主分区；同时每项由MBR的16字节扩大到128字节，描述每个分区的开始扇区和结束扇区都用8个字节，因而最多支持2的64次方的扇区数，即支持最大约9.4ZB大小的分区。分区表项的具体内容如下：</p><p><img src="https://img-blog.csdn.net/20180721165455982?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3piZ2poeTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" /></p><h4 id="mbr-vs-gpt"><a class="markdownIt-Anchor" href="#mbr-vs-gpt"></a> MBR VS. GPT</h4><p>MBR的缺点之一是不支持大于2.2T的分区，而GPT可以支持18EB的硬盘；</p><p>MBR还有一个缺点是限制磁盘不能超过4个主分区或者3主分区+1扩展分区（包含随意数目的逻辑分区），而GPT则没有此限制。</p><p>GPT也有缺点，GPT分区硬盘在修复磁盘坏轨、做资料恢复、系统还原等任务时都会遇到麻烦，MBR则较为方便。</p><p>GPT 定义是 Intel提出的用以替代BIOS以实现对更多硬件的支持的规范：EFI( Extensible Firmware Interface ) 的一部分。例如，使用EFI/UEFI就可以引导GPT分区下的系统，可以将系统安装到2T容量以上的硬盘中。</p><p>考虑到兼容性，在EFI/UEFI中可以设置Legacy（传统）模式，从而引导MBR分区下的系统了。</p><p>而BIOS只可以引导MBR分区下的系统，不可以引导GPT分区下的系统（但支持GPT分区的硬盘做数据盘）。</p><h4 id="查看分区类型"><a class="markdownIt-Anchor" href="#查看分区类型"></a> 查看分区类型</h4><p><em>Windows系统下</em></p><p>在“此电脑–管理–磁盘管理”中右键查看“磁盘0”或“磁盘1”属性，在“卷”选项卡下的“磁盘分区形式”即是该磁盘分区类型。</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/About_Disks/20181030165722528.png" alt="" /></p><p><em>Linux系统下</em></p><p>使用命令查询：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo parted -l</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/About_Disks/20181030165744755.png" alt="" /></p><p>Partition Table显示gpt则说明是GPT分区表，显示msdos则说明是MBR分区表。</p><h4 id="mbr与gpt的选择"><a class="markdownIt-Anchor" href="#mbr与gpt的选择"></a> MBR与GPT的选择</h4><h6 id="1-作为系统盘"><a class="markdownIt-Anchor" href="#1-作为系统盘"></a> 1. 作为系统盘</h6><p>在Intel的大力推动下，现在很多PC、服务器出厂就是使用EFI/UEFI来启动系统的，磁盘的分区表也是GPT。</p><p>如果换上一块全新的磁盘，并希望使用MBR分区表，则需要在EFI/UEFI中切换Legacy模式，否则安装系统时会出错，例如：“Windows 无法安装到此磁盘。选定磁盘不是 GPT 分区类型的磁盘”。</p><h6 id="2-作为数据盘"><a class="markdownIt-Anchor" href="#2-作为数据盘"></a> 2. 作为数据盘</h6><p>GPT或是MBR都可以被在EFI/UEFI引导的系统下使用，没有差别。如果需要大于2.2T的分区或是更高的主分区数要求，则选择GPT。</p><p><strong>总的来说</strong>，作为系统盘，如果需要在2.2T容量以上的分区安装系统，或者对主分区数有更高要求，就必须使用GPT+EFI/UEFI，否则选择GPT+EFI/UEFI、MBR+BIOS或者MBR+EFI/UEFI（Legacy）都是可以的；而作为数据盘，选择的随意性更大，不需要考虑EFI/UEFI的影响。</p><p>我的建议是分区方式最好不要转换，可能会造成文件的丢失，如果确有必要，可以通过傲梅分区助手、DiskGenius来辅助，切不可使用Windows系统磁盘管理中的转换或是Linux安装系统的转换功能。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;注：下文不特别区分“磁盘”、“硬盘”两个概念，认为“固态硬盘”也属于“磁盘”的一种。&lt;/p&gt;
&lt;h2 id=&quot;磁盘接口&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#磁盘接口&quot;&gt;&lt;/a&gt; 磁盘接口&lt;/h2&gt;
&lt;p&gt;IDE（Integrated Drive Electronics），即电子集成驱动器，是曾经主流的磁盘接口（同时也作为光驱的接口）。IDE又称ATA（Advanced Technology Attachment），即“高级技术附件”，在SATA（Serial ATA）出现后改名为PATA（Parallel ATA）。&lt;/p&gt;
&lt;p&gt;SCSI（Small Computer System Interface），即小型计算机系统接口，可以连接硬盘、软驱、光驱、打印机等设备。SCSI出现的原因主要是因为IDE接口的硬盘转速太慢，传输速率太低。从原理层面看，SCSI与IDE一样使用的是并行技术，因此在SAS（Serial Attached SCSI）出现后SCSI就通常被称为并行SCSI了。&lt;/p&gt;
&lt;p&gt;此后又出现了SATA（Serial Advanced Technology Attachment，串行高级技术附件）、SAS（Serial Attached SCSI）即串行连接SCSI 等接口，使用了串行技术，提高了数据传输速率。&lt;/p&gt;
&lt;p&gt;我们可以将，SCSI和SAS划归为另一个系列，SCSI和SAS价格较高，在各自的年代都是用在高级的服务器上的，私人电脑上较少使用。而将IDE和SATA划归为一个系列，它们用于一般电脑或PC上，SATA在当下非常普遍。&lt;/p&gt;</summary>
    
    
    
    <category term="Computer" scheme="https://forskamse.github.io/categories/Computer/"/>
    
    <category term="Basics of Computers" scheme="https://forskamse.github.io/categories/Computer/Basics-of-Computers/"/>
    
    
    <category term="Computer" scheme="https://forskamse.github.io/tags/Computer/"/>
    
    <category term="Operation&amp;Maintenance" scheme="https://forskamse.github.io/tags/Operation-Maintenance/"/>
    
  </entry>
  
  <entry>
    <title>Linux文件系统</title>
    <link href="https://forskamse.github.io/2018/07/17/2018-07-17-Linux%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    <id>https://forskamse.github.io/2018/07/17/2018-07-17-Linux%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/</id>
    <published>2018-07-17T08:37:57.000Z</published>
    <updated>2021-06-24T14:14:45.431Z</updated>
    
    <content type="html"><![CDATA[<h4 id="文件系统"><a class="markdownIt-Anchor" href="#文件系统"></a> 文件系统</h4><p>计算机的文件系统是一种存储和组织计算机数据的方法，借助于文件系统，用户或程序对文件的访问和查找变得容易。 Linux支持的文件系统格式有：Ext2, Ext3, Ext4, ReiserFS, Xfs, Btrfs, FAT, FAT32, NTFS等。本文中，我并不打算展开对这些文件系统的详细解释，只是简单说出以下这些结论：</p><p>Ext2, Ext3, Ext4是Linux系统上最常用的文件系统，发展到Ext4时已经十分稳定，没有特别要求时，一般都可以使用；</p><p>ReiserFS是用B+树作为数据结构的文件系统，在处理小文件时有较好的性能，在实践中，ReiserFS在处理文件小于1k小文件时，甚至效率可以比ext3快约10倍;</p><p>XFS使用64位管理空间，在多文件、大文件系统、空间利用率等方面相比Ext4更有优势。从CentOS 7开始，默认的文件系统就由此前的Ext4改为XFS了，由于文件规模的不断增大，日后Ext4可能会被XFS所取代。</p><p>Btrfs官方宣称其为“下一代文件系统”，虽然从理念上看Btrfs确实可能存在不错的效果，但截至目前，它的性能表现还是太差了，不建议使用。</p><p>【Ext4、XFS、Btrfs的详细对比，感兴趣的读者可以看看这篇Benchmark：<a href="http://www.ilsistemista.net/index.php/linux-a-unix/6-linux-filesystems-benchmarked-ext3-vs-ext4-vs-xfs-vs-btrfs.html?start=1">EXT3 vs EXT4 vs XFS vs BTRFS linux filesystems benchmark</a>】</p><span id="more"></span><p>Linux虽然支持FAT、FAT32、NTFS，但仅仅是为了兼容性，这三个文件系统很容易产生磁盘碎片（尽管NTFS上已有不小改善），Linux系统下一般是不会轻易使用的。</p><h4 id="挂载点"><a class="markdownIt-Anchor" href="#挂载点"></a> 挂载点</h4><p>挂载点是linux中的磁盘文件系统的入口目录。</p><p><img src="https://gitee.com/forskamse/forskamse-open-images/raw/main/blog/Jul_2018/Linux_File_System/20180721165625481.jpeg" alt="" /></p><p>挂载点与其功能描述如下：</p><table><thead><tr><th>/</th><th>根目录，存放系统命令和用户数据等（如果下面挂载点没有单独的分区，它们都将在根目录的分区中）</th></tr></thead><tbody><tr><td>/boot</td><td>boot   loader 的静态链接文件，存放与Linux启动相关的程序</td></tr><tr><td>/home</td><td>用户目录，存放普通用户的数据</td></tr><tr><td>/tmp</td><td>临时文件</td></tr><tr><td>/usr</td><td>是Red Hat Linux系统存放软件的地方,如有可能应将最大空间分给它：      /usr/local 自已安装程序安装在此   /usr/X1186           X-Windows目录，存放一些X-Windows的配置文件   /usr/include        系统头文件，存储一些C语言的头文件   /usr/src             Linux内核源代码，Linux系统所安装的内核源代码都保存在此   /usr/bin            对/bin目录的一些补充   /usr/sbin          对/sbin目录的一些补充   /usr/share/doc   用户文档</td></tr><tr><td>/var</td><td>不断变化的数据，服务器的一些服务、日志放在下面：   /var/www：一般WEB存放网页的目录   /var/mail：postfix邮件的存放邮件的目录   /var/log：系统日志记录   /var/spool：存放一些邮件、新闻、打印队列等。</td></tr><tr><td>/opt</td><td>（Option可选的）附加的应用程序软件包</td></tr><tr><td>/bin</td><td>基本命令执行文件</td></tr><tr><td>/dev</td><td>设备文件</td></tr><tr><td>/etc</td><td>主机特定的系统配置</td></tr><tr><td>/lib</td><td>基本共享库以及内核模块</td></tr><tr><td>/media</td><td>用于移动介质的挂载点</td></tr><tr><td>/mnt</td><td>用于临时挂载文件系统或者别的硬件设备（如光驱、软驱）</td></tr><tr><td>/proc</td><td>系统信息的虚拟目录(2.4 和 2.6 内核)，这些信息是在内存中，由系统自己产生的。</td></tr><tr><td>/root</td><td>root   用户的目录</td></tr><tr><td>/sbin</td><td>基本系统命令执行文件</td></tr><tr><td>/sys</td><td>系统信息的虚拟目录(2.6 内核)</td></tr><tr><td>/srv</td><td>系统提供的用于 service 的数据</td></tr><tr><td>/lost+found</td><td>这个目录在大多数情况下都是空的。但是如果你正在工作突然停电，或是没有用正常方式关机，在你重新启动机器的时候，有些文件就会找不到应该存放的地方，对于这些文件，系统将他们放在这个目录下。</td></tr></tbody></table><p>当然上面这么多挂载点，实际上是没有比较每个目录都单独进行挂载，我们只需要根据自己的实际使用需要对个别目录进行挂载，这样系统结构看起来也会精简很多。最少的时候，我们只需要挂载/就可以了（当然这样并不好）。</p><h4 id="分区"><a class="markdownIt-Anchor" href="#分区"></a> 分区</h4><p>根据挂载点的不同，对磁盘进行分区，选择最合适的文件系统，可以使计算机的性能、管理达到最优。</p><p>分区有很多的优点，例如：</p><p>1）保护数据；假如误操作，有分区的情况下就可能保护一部分数据免受误操作的影响；重装操作系统时，如果原先的系统中/home与/两个挂载点是对应着两个不同分区时，/home目录就不会受到影响；</p><p>2）针对不同挂载点的特性选择文件系统，开启不同的挂载选项（如是否需要即时同步，是否开启日志，是否启用压缩）以更好地发挥性能，比如对/var使用Reiserfs（这里面的文件通常小而繁杂），对/home使用XFS（超大容量支持可能是用户文件比较需求的），对/使用Ext4（更加稳定）。</p><p>3）分区可以缩小硬盘搜索范围，提高效率。</p><p>我举一例比较典型的分区方案：</p><table><thead><tr><th>挂载点</th><th>分区</th><th>文件系统</th><th>分配详情</th></tr></thead><tbody><tr><td>/boot</td><td>启动分区</td><td>Ext4</td><td>只需要几百m即可，可以容纳下两三个内核足矣。</td></tr><tr><td>/swap</td><td>交换分区</td><td>Swap</td><td>物理内存的1.5-2倍，物理内存够大也可不分配</td></tr><tr><td>/</td><td>根分区</td><td>Ext4</td><td>桌面系统给个100G~200G足矣。</td></tr><tr><td>/home</td><td>家分区</td><td>XFS</td><td>剩下的可以全部分配给家分区</td></tr></tbody></table><h4 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h4><p><a href="https://linux.cn/article-7083-1.html">https://linux.cn/article-7083-1.html</a></p><p><a href="http://my.oschina.net/leejun2005/blog/290073">http://my.oschina.net/leejun2005/blog/290073</a></p><p><a href="http://wuchong.me/blog/2014/07/19/linux-file-system/">http://wuchong.me/blog/2014/07/19/linux-file-system/</a></p>]]></content>
    
    
    <summary type="html">&lt;h4 id=&quot;文件系统&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#文件系统&quot;&gt;&lt;/a&gt; 文件系统&lt;/h4&gt;
&lt;p&gt;计算机的文件系统是一种存储和组织计算机数据的方法，借助于文件系统，用户或程序对文件的访问和查找变得容易。 Linux支持的文件系统格式有：Ext2, Ext3, Ext4, ReiserFS, Xfs, Btrfs, FAT, FAT32, NTFS等。本文中，我并不打算展开对这些文件系统的详细解释，只是简单说出以下这些结论：&lt;/p&gt;
&lt;p&gt;Ext2, Ext3, Ext4是Linux系统上最常用的文件系统，发展到Ext4时已经十分稳定，没有特别要求时，一般都可以使用；&lt;/p&gt;
&lt;p&gt;ReiserFS是用B+树作为数据结构的文件系统，在处理小文件时有较好的性能，在实践中，ReiserFS在处理文件小于1k小文件时，甚至效率可以比ext3快约10倍;&lt;/p&gt;
&lt;p&gt;XFS使用64位管理空间，在多文件、大文件系统、空间利用率等方面相比Ext4更有优势。从CentOS 7开始，默认的文件系统就由此前的Ext4改为XFS了，由于文件规模的不断增大，日后Ext4可能会被XFS所取代。&lt;/p&gt;
&lt;p&gt;Btrfs官方宣称其为“下一代文件系统”，虽然从理念上看Btrfs确实可能存在不错的效果，但截至目前，它的性能表现还是太差了，不建议使用。&lt;/p&gt;
&lt;p&gt;【Ext4、XFS、Btrfs的详细对比，感兴趣的读者可以看看这篇Benchmark：&lt;a href=&quot;http://www.ilsistemista.net/index.php/linux-a-unix/6-linux-filesystems-benchmarked-ext3-vs-ext4-vs-xfs-vs-btrfs.html?start=1&quot;&gt;EXT3 vs EXT4 vs XFS vs BTRFS linux filesystems benchmark&lt;/a&gt;】&lt;/p&gt;</summary>
    
    
    
    <category term="Computer" scheme="https://forskamse.github.io/categories/Computer/"/>
    
    <category term="Basics of Computers" scheme="https://forskamse.github.io/categories/Computer/Basics-of-Computers/"/>
    
    
    <category term="Computer" scheme="https://forskamse.github.io/tags/Computer/"/>
    
    <category term="Operation&amp;Maintenance" scheme="https://forskamse.github.io/tags/Operation-Maintenance/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络重要论文资源合辑</title>
    <link href="https://forskamse.github.io/2018/06/20/2018-06-20-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8D%E8%A6%81%E8%AE%BA%E6%96%87%E8%B5%84%E6%BA%90%E5%90%88%E8%BE%91/"/>
    <id>https://forskamse.github.io/2018/06/20/2018-06-20-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%87%8D%E8%A6%81%E8%AE%BA%E6%96%87%E8%B5%84%E6%BA%90%E5%90%88%E8%BE%91/</id>
    <published>2018-06-20T06:00:00.000Z</published>
    <updated>2021-06-24T14:13:17.815Z</updated>
    
    <content type="html"><![CDATA[<h2 id="卷积神经网络的前身与早期发展"><a class="markdownIt-Anchor" href="#卷积神经网络的前身与早期发展"></a> <strong>卷积神经网络的前身与早期发展：</strong></h2><ul><li><strong>1980年日本学者福岛邦彦（Kunihiko Fukushima）提出的神经认知机模型（Neocognitron）</strong><br />Fukushima K, Miyake S. Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition[M].Competition and cooperation in neural nets. Springer, Berlin, Heidelberg, 1982: 267-285.</li><li><strong>1989年Yann LeCun提出第一个真正意义上的CNN：LeNet 1989</strong><br />LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to handwritten zip code recognition[J]. Neural computation, 1989, 1(4): 541-551.</li><li><strong>1998年Yann LeCun在其博士论文中详细介绍了LeNet（又称LeNet-5），影响力巨大</strong><br />LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324.</li></ul><span id="more"></span><h2 id="2012年以来卷积神经网络迎来迅猛发展阶段"><a class="markdownIt-Anchor" href="#2012年以来卷积神经网络迎来迅猛发展阶段"></a> <strong>2012年以来卷积神经网络迎来迅猛发展阶段：</strong></h2><ul><li><strong>2012年ILSVRC冠军：AlexNet，掀起深度学习计算机视觉狂潮</strong><br />Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C].Advances in neural information processing systems. 2012: 1097-1105.</li><li><strong>2013年ILSVRC冠军：ZFNet</strong><br />Zeiler M D, Fergus R. Visualizing and understanding convolutional networks[C].European conference on computer vision. Springer, Cham, 2014: 818-833.</li><li><strong>2014年ILSVRC冠军：GoogLeNet，提出Inception结构</strong><br />Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]. Cvpr, 2015.</li><li><strong>2014年ILSVRC亚军：VGGNet，亮点是对网络深度的研究</strong><br />Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</li><li><strong>2015年ILSVRC冠军：ResNet，提出Residual结构</strong><br />He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C].Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</li></ul><h2 id="卷积神经网络结合改进与瓶颈阶段"><a class="markdownIt-Anchor" href="#卷积神经网络结合改进与瓶颈阶段"></a> <strong>卷积神经网络结合改进与瓶颈阶段：</strong></h2><p><em><strong>合理结合Inception结构与Residual结构的卷积神经网络已经能够达到令人满意的特征提取效果，但是在解释性上却没有更深一步进展。</strong></em></p><ul><li><strong>2016年Google团队结合了Inception结构与Residual 结构，提出Inception-Residual Net</strong><br />Szegedy C, Ioffe S, Vanhoucke V, et al. Inception-v4, inception-resnet and the impact of residual connections on learning[C].AAAI. 2017, 4: 12.</li><li><strong>2016年何凯明提出新的ResNet的想法：Identity Mapping</strong><br />He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C].European Conference on Computer Vision. Springer, Cham, 2016: 630-645.</li><li><strong>2017年DenseNet</strong><br />Huang G, Liu Z, Weinberger K Q, et al. Densely connected convolutional networks[C].Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, 1(2): 3.</li></ul><h2 id="轻量级卷积神经网络发展阶段"><a class="markdownIt-Anchor" href="#轻量级卷积神经网络发展阶段"></a> <strong>轻量级卷积神经网络发展阶段：</strong></h2><p><em><strong>2016年以来，卷积神经网络开始往轻量化发展，为视觉深度学习模型在移动设备上的应用提供条件。</strong></em></p><ul><li><strong>2016年MobileNet</strong><br />Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications[J]. arXiv preprint arXiv:1704.04861, 2017.</li><li><strong>2016年ShuffleNet</strong><br />Zhang X, Zhou X, Lin M, et al. Shufflenet: An extremely efficient convolutional neural network for mobile devices[J]. arXiv preprint arXiv:1707.01083, 2017.</li><li><strong>2016年Xception</strong>【注：Xception目标并不是使卷积神经网络轻量化，而是在不增加网络复杂度的情况下提升性能，但其中使用的depthwise convolution思想是MobileNet等轻量级卷积神经网络的关键，故也列在这里】<br />Chollet F. Xception: Deep learning with depthwise separable convolutions[J]. arXiv preprint, 2017: 1610.02357.</li><li><strong>2016年ResNeXt</strong>【注：ResNeXt也是为了在不增加网络复杂度的情况下提升性能，列在此处的原因与Xception相同】<br />Xie S, Girshick R, Dollár P, et al. Aggregated residual transformations for deep neural networks[C].Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017: 5987-5995.</li></ul><p><strong>论文合集GitHub地址：<a href="https://github.com/forskamse/CNN">CNN-Papers</a></strong></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;卷积神经网络的前身与早期发展&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#卷积神经网络的前身与早期发展&quot;&gt;&lt;/a&gt; &lt;strong&gt;卷积神经网络的前身与早期发展：&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1980年日本学者福岛邦彦（Kunihiko Fukushima）提出的神经认知机模型（Neocognitron）&lt;/strong&gt;&lt;br /&gt;
Fukushima K, Miyake S. Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition[M].Competition and cooperation in neural nets. Springer, Berlin, Heidelberg, 1982: 267-285.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1989年Yann LeCun提出第一个真正意义上的CNN：LeNet 1989&lt;/strong&gt;&lt;br /&gt;
LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to handwritten zip code recognition[J]. Neural computation, 1989, 1(4): 541-551.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1998年Yann LeCun在其博士论文中详细介绍了LeNet（又称LeNet-5），影响力巨大&lt;/strong&gt;&lt;br /&gt;
LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324.&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://forskamse.github.io/categories/Deep-Learning/"/>
    
    <category term="Neural Networks" scheme="https://forskamse.github.io/categories/Deep-Learning/Neural-Networks/"/>
    
    <category term="CNN Special Column" scheme="https://forskamse.github.io/categories/Deep-Learning/Neural-Networks/CNN-Special-Column/"/>
    
    
    <category term="CNN" scheme="https://forskamse.github.io/tags/CNN/"/>
    
  </entry>
  
</feed>
